\documentclass[parskip=full]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\hypersetup{
pdftitle={xyz}
}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[nonumberlist]{glossaries}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{scalerel,amssymb}
\def\msquare{\mathord{\scalerel*{\Box}{gX}}}

\title{Graph Neural Networks}
\subtitle{Script based on CS224W}
\author{Unity05}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}

\section{Message Passing Graph Neural Networks}

\subsection{Introduction}
The idea behind message passing GNNs is \textbf{k - hop neighborhood aggregation}.
One single GNN layer can be looked at as one single hop.

A GNN layer mainly consists of two parts: \textbf{Message Computation} and \textbf{Aggregation}.

\subsubsection{Message Computation}
Each node computes a message based on it's embedding in the previous layer.
\[m_u^{(l)} = \phi^{(l)}\left(h_u^{(l-1)}\right)\]
\begin{center}
\begin{tabular}{l c l}
	$m_u^{(l)}$ & \dots & message of node u in layer l \\
	$\phi^{(l)}$ & \dots & message computation function of layer l \\
	$h_u^{(l - 1)}$ & \dots & node u's embedding in layer l - 1
\end{tabular}
\end{center}

\subsubsection{Aggregation}
Node v's new embedding is computed by aggregating its own message as well as all of its neighbor node's messages.
\[h_v^{(l)} = \sigma\left(\msquare^{(l)}\left(\{m_u^{(l)} \mid u \in N(v)\}, m_v^{(l)}\right)\right)\]
\begin{center}
\begin{tabular}{l c l}
	$\sigma$ & \dots & nonlinear activation function \\
	$h_v^{(l)}$ & \dots & node v's new embedding in layer l - 1 \\
	$\msquare^{(l)}$ & \dots & aggregation function of layer l \\
	$m_u^{(l)}$ & \dots & message of node u in layer l \\
	$N(v)$ & \dots & neighborhood of node v
\end{tabular}
\end{center}


\subsection{Popular GNN Layers}

\subsubsection{\href{https://arxiv.org/pdf/1609.02907.pdf}{Graph Convolutional Networks (GCN)}}
\begin{itemize}
	\item[] \textbf{Message Computation}\newline
		Embeddings are passed through a linear layer (transformation with 		weight matrix). Normalized by node degrees.
		\[m_u^{(l)} = \left(W^{(l)} \cdot h_u^{(l-1)}\right)\]
	\item[] \textbf{Aggregation}\newline
		\[h_v^{(l)} = \sigma\left(\sum_{u \in N(v) \cup \{v\}} \frac{1}{\sqrt{deg(u)} \cdot \sqrt{deg(v)}} \cdot m_u^{(l)}\right)\]
\end{itemize}

\subsubsection{\href{https://arxiv.org/pdf/1710.10903.pdf}{Graph Attention Networks (GAT)}}
\begin{itemize}
	\item[] \textbf{Message Computation}\newline
		No difference to GCN.
		\[m_u^{(l)} = \left(W^{(l)} \cdot h_u^{(l-1)}\right)\]
	\item[] \textbf{Aggregation}\newline
		Weighted summation of messages normalized by attention weights.
		\[h_v^{(l)} = \sigma\left(\sum_{u \in N(v) \cup \{v\}} \alpha_{vu} \cdot m_u^{(l)}\right)\]
		Computation of $\alpha_{vu}$ with attention mechanism a:
		\[\alpha_{vu} = \frac{exp(e_{vu})}{\sum_{k \in N(v)} exp(e_{vk})}\]
		\[e_{vu} = a\left(W^{(l)}h_u^{(l-1)}, W^{(l)}h_v^{(l-1)}\right)\]
\end{itemize}

\subsubsection{\href{https://arxiv.org/pdf/1810.00826.pdf}{Graph Isomorphism Network (GIN)}}\label{GIN}
We don't split up in Message Computation and Aggregation as MLP models both functions $\phi$ and $f$.
\[h_v^{(l)} = MLP^{(l)}\left(\left(1 + \epsilon^{(l)}\right) \cdot h_v^{(l-1)} + \sum_{u \in N(v)} h_u^{(l-1)}\right)\]


\subsection{Expressiveness of GNNs}
To assess the expressiveness of a GNN we have to look at its \textbf{computational graph}. It is important to keep in mind that the computational graph can only distinguish different node features not node IDs. Therefore, they are identical to subtrees rooted at the respective nodes.\newline
Hence, we want GNNs to \textbf{map subtrees injectively}. This means we need to retain information about neighborhood structure. As the same feature yields to the same message we achieve this by using \textbf{injectiv aggregation functions}.

\subsubsection{Mean - Pool}
Cannot distinguish different multi-sets with the same embedding proportion.

\subsubsection{Max - Pool}
Cannot distinguish different sets with the same embeddings.

\subsubsection{Injective Multi-Set Function}
Any multi-set function can be expressed as:
\[\phi\left(\sum_{x \in X} f(x)\right)\]
According to the \href{https://deeplearning.cs.cmu.edu/F21/document/readings/Hornik_Stinchcombe_White.pdf}{\textbf{universal approximation theorem}}, any function $\phi$ and $f$ can be modeled with an MLP. As MLPs can model compositions of functions, we end up with:
\[MLP\left(\sum_{x \in X} x\right)\]
Transferring this to the domain of GNNs it yields \textbf{GIN(\ref{GIN}) as the most expressive GNN}.


\section{Graph Generation}

\subsection{Network Properties}

\subsubsection{Degree Distribution}
The degree distribution \textbf{$P(k)$} is the probability that a random node has degree k.\newline
Computation:
\[N_k = \text{\#nodes with degree k}\]
\[P(k) = \frac{N_k}{N}\]

\subsubsection{Clustering Coefficient}
How connected the neighborhood of node $i$ with degree $k_i$ is. It basically measures, how many edges out of all possible edges exist.
\[C_i = \frac{2e_i}{k_i\left(k_i-1\right)}\]
\[C = \frac{1}{N} \sum_{i}^{N} C_i\]
\begin{center}
\begin{tabular}{l c l}
	$C_i$ & \dots & clustering coefficient \\
	$e_i$ & \dots & total number of edges in neighborhood of node $i$
\end{tabular}
\end{center}

\subsubsection{Connectivity}
Connectivity is the size of the \textbf{largest connected component}.
This largest component is called the \textbf{giant component}.
Can be found by i.e. BFS.

\subsubsection{Path Length}
\begin{itemize}
	\item[] \textbf{Diameter}\newline
		\[max_{u,v}d(u,v)\]
		\begin{center}
			$d(u,v)$ \dots length of shortest path between u and v
		\end{center}
	\item[] \textbf{Average Path Length}\newline
		\[\overline{h} = \frac{1}{2E_{max}}\sum_{i,j \neq i} h_{ij}\]
		\begin{center}
		\begin{tabular}{l c l}
			$E_{max}$ & \dots & max number of edges $\left(\frac{n(n-1)}{2}\right)$ \\
			$h_{ij}$ & \dots & $d(i,j)$
		\end{tabular}
		\end{center}
\end{itemize}

\subsubsection{Expansion}
Expansion $\alpha$ is defined as
\[\alpha = \min_{S \subseteq V}\frac{\text{\#edges leaving S}}{min(\vert S \vert, \vert V \setminus S \vert)}\]
and can be interpreted as the minimum average number of edges that ties a node $u$ of set $S$ to the set $V \setminus S$. This corresponds to a kind of \textbf{robustness of the graph}.

\subsection{Erdös-Renyi Random Graphs}
An Erdös-Renyi Random Graph \textbf{$G_{np}$} is an undirected graph with \textbf{n nodes}. Each \textbf{edge (u,v)} has probability \textbf{$p$} to exist.

\subsubsection{Degree Distribution}
The degree distribution \textbf{$P(k)$} for each node is \textbf{binomial}. We use $n-1$ instead of $n$ as we don't allow self loops.
\[P(k) = \binom{n-1}{k}p^k(1-p)^{n-1-k}\]

\subsubsection{Clustering Coefficient}
As the \# edge distribution is binomial:
\[\mathbf{E}[e_i] = p \cdot \frac{k_i(k_i - 1)}{2}\]
\[\Longrightarrow \mathbf{E}[C_i] = \frac{2\mathbf{E}[e_i]}{k_i(k_i-1)} = \frac{p \cdot k_i(k_i-1)}{k_i(k_i-1} = p = \frac{\overline{k}}{n-1} \approx \frac{\overline{k}}{n}\]

\subsection{Small-World Model}
Small world models try to achieve \textbf{low diameter} while retaining a \textbf{high clustering coefficient}.
\begin{itemize}
	\item start with regular grid $\Rightarrow$ high clustering coefficient
	\item change target of some edges randomly $\Rightarrow$ low diameter
\end{itemize}

\subsection{Kronecker Graphs}

\subsubsection{Kronecker Product}
Let A and B be matrices.
\[C = A \otimes B = 
\begin{pmatrix}
a_{1,1}B & \cdots & a_{1,m}B \\
\vdots & \vdots & \vdots \\
a_{n,1}B & \cdots & a_{n,m}B
\end{pmatrix}\]
For \textbf{recursive graph structurs} we apply the Kronecker Product \textbf{iteratively}. Different matrices $K_1$, $K_1^{'}$, $\ldots$ are possible.
\[K_1^{m} = K_m = K_{m-1} \otimes K_1\]

\end{document}
