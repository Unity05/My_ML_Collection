\documentclass[parskip=full]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\hypersetup{
pdftitle={xyz}
}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[nonumberlist]{glossaries}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{scalerel,amssymb}
\def\msquare{\mathord{\scalerel*{\Box}{gX}}}

\title{Graph Neural Networks}
\subtitle{Script based on CS224W}
\author{Unity05}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}

\section{Message Passing Graph Neural Networks}

\subsection{Introduction}
The idea behind message passing GNNs is \textbf{k - hop neighborhood aggregation}.
One single GNN layer can be looked at as one single hop.

A GNN layer mainly consists of two parts: \textbf{Message Computation} and \textbf{Aggregation}.

\subsubsection{Message Computation}
Each node computes a message based on it's embedding in the previous layer.
\[m_u^{(l)} = \phi^{(l)}\left(h_u^{(l-1)}\right)\]
\begin{center}
\begin{tabular}{l c l}
	$m_u^{(l)}$ & \dots & message of node u in layer l \\
	$\phi^{(l)}$ & \dots & message computation function of layer l \\
	$h_u^{(l - 1)}$ & \dots & node u's embedding in layer l - 1
\end{tabular}
\end{center}

\subsubsection{Aggregation}
Node v's new embedding is computed by aggregating its own message as well as all of its neighbor node's messages.
\[h_v^{(l)} = \sigma\left(\msquare^{(l)}\left(\{m_u^{(l)} \mid u \in N(v)\}, m_v^{(l)}\right)\right)\]
\begin{center}
\begin{tabular}{l c l}
	$\sigma$ & \dots & nonlinear activation function \\
	$h_v^{(l)}$ & \dots & node v's new embedding in layer l - 1 \\
	$\msquare^{(l)}$ & \dots & aggregation function of layer l \\
	$m_u^{(l)}$ & \dots & message of node u in layer l \\
	$N(v)$ & \dots & neighborhood of node v
\end{tabular}
\end{center}


\subsection{Popular GNN Layers}

\subsubsection{Graph Convolutional Networks (GCN)}
\begin{itemize}
	\item[] \textbf{Message Computation}\newline
		Embeddings are passed through a linear layer (transformation with 		weight matrix). Normalized by node degrees.
		\[m_u^{(l)} = \left(W^{(l)} \cdot h_u^{(l-1)}\right)\]
	\item[] \textbf{Aggregation}\newline
		\[h_v^{(l)} = \sigma\left(\sum_{u \in N(v) \cup \{v\}} \frac{1}{\sqrt{deg(u)} \cdot \sqrt{deg(v)}} \cdot m_u^{(l)}\right)\]
\end{itemize}

\subsubsection{Graph Attention Networks (GAT)}
\begin{itemize}
	\item[] \textbf{Message Computation}\newline
		No difference to GCN.
		\[m_u^{(l)} = \left(W^{(l)} \cdot h_u^{(l-1)}\right)\]
	\item[] \textbf{Aggregation}\newline
		Weighted summation of messages normalized by attention weights.
		\[h_v^{(l)} = \sigma\left(\sum_{u \in N(v) \cup \{v\}} \alpha_{vu} \cdot m_u^{(l)}\right)\]
		Computation of $\alpha_{vu}$ with attention mechanism a:
		\[\alpha_{vu} = \frac{exp(e_{vu})}{\sum_{k \in N(v)} exp(e_{vk})}\]
		\[e_{vu} = a\left(W^{(l)}h_u^{(l-1)}, W^{(l)}h_v^{(l-1)}\right)\]
\end{itemize}
\end{document}