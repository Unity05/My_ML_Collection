\documentclass[parskip=full]{scrartcl}
\usepackage[utf8]{inputenc}
%\usepackage{glossaries}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\hypersetup{
pdftitle={xyz}
}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{csquotes}
\usepackage[nonumberlist]{glossaries}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{scalerel,amssymb}
\def\msquare{\mathord{\scalerel*{\Box}{gX}}}
\DeclareMathOperator*{\argmax}{arg\, max}

%\usepackage[
%sortcites,
%backend=biber,
%style=alphabetic, %numeric-comp, %style=numeric alphabetic
%defernumbers=true, %bibstyle=authortitle
%bibstyle=alphabetic,
%maxbibnames=999, maxcitenames=2
%]{biblatex}
%\usepackage{natbib}
\usepackage{biblatex}
\addbibresource{references.bib}


\makenoidxglossaries

\newglossaryentry{Fixed Split}
{
	name={Fixed Split},
	description={We split the dataset into Training Set, Validation Set and Test Set}
}

\newglossaryentry{Random Split}
{
	name={Random Split},
	description={We randomly split the dataset into Training Set, Validation Set and Test Set. We then take the mean performance over multiple random seeds}
}


\title{Graph Neural Networks}
\subtitle{Script based on CS224W: Winter 2021 \cite{cs224w-notes}}
\author{Unity05}

\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Message Passing Graph Neural Networks}

\subsection{Introduction}
The idea behind message passing GNNs is \textbf{k - hop neighborhood aggregation}.
One single GNN layer can be looked at as one single hop.

A GNN layer mainly consists of two parts: \textbf{Message Computation} and \textbf{Aggregation}.

\subsubsection{Message Computation}
Each node computes a message based on it's embedding in the previous layer.
\[m_u^{(l)} = \phi^{(l)}\left(h_u^{(l-1)}\right)\]
\begin{center}
\begin{tabular}{l c l}
	$m_u^{(l)}$ & \dots & message of node u in layer l \\
	$\phi^{(l)}$ & \dots & message computation function of layer l \\
	$h_u^{(l - 1)}$ & \dots & node u's embedding in layer l - 1
\end{tabular}
\end{center}

\subsubsection{Aggregation}
Node v's new embedding is computed by aggregating its own message as well as all of its neighbor node's messages.
\[h_v^{(l)} = \sigma\left(\msquare^{(l)}\left(\{m_u^{(l)} \mid u \in N(v)\}, m_v^{(l)}\right)\right)\]
\begin{center}
\begin{tabular}{l c l}
	$\sigma$ & \dots & nonlinear activation function \\
	$h_v^{(l)}$ & \dots & node v's new embedding in layer l - 1 \\
	$\msquare^{(l)}$ & \dots & aggregation function of layer l \\
	$m_u^{(l)}$ & \dots & message of node u in layer l \\
	$N(v)$ & \dots & neighborhood of node v
\end{tabular}
\end{center}


\subsection{Popular GNN Layers}

\subsubsection{Graph Convolutional Networks (GCN) \cite{kipf2017semisupervised}}
\begin{itemize}
	\item[] \textbf{Message Computation}\newline
		Embeddings are passed through a linear layer (transformation with 		weight matrix). Normalized by node degrees.
		\[m_u^{(l)} = \left(W^{(l)} \cdot h_u^{(l-1)}\right)\]
	\item[] \textbf{Aggregation}\newline
		\[h_v^{(l)} = \sigma\left(\sum_{u \in N(v) \cup \{v\}} \frac{1}{\sqrt{deg(u)} \cdot \sqrt{deg(v)}} \cdot m_u^{(l)}\right)\]
\end{itemize}

\subsubsection{Graph Attention Networks (GAT) \cite{veličković2018graph}}
\begin{itemize}
	\item[] \textbf{Message Computation}\newline
		No difference to GCN.
		\[m_u^{(l)} = \left(W^{(l)} \cdot h_u^{(l-1)}\right)\]
	\item[] \textbf{Aggregation}\newline
		Weighted summation of messages normalized by attention weights.
		\[h_v^{(l)} = \sigma\left(\sum_{u \in N(v) \cup \{v\}} \alpha_{vu} \cdot m_u^{(l)}\right)\]
		Computation of $\alpha_{vu}$ with attention mechanism a:
		\[\alpha_{vu} = \frac{exp(e_{vu})}{\sum_{k \in N(v)} exp(e_{vk})}\]
		\[e_{vu} = a\left(W^{(l)}h_u^{(l-1)}, W^{(l)}h_v^{(l-1)}\right)\]
\end{itemize}

\subsubsection{Graph Isomorphism Network (GIN) \cite{xu2019powerful}}\label{GIN}
We don't split up in Message Computation and Aggregation as MLP models both functions $\phi$ and $f$.
\[h_v^{(l)} = MLP^{(l)}\left(\left(1 + \epsilon^{(l)}\right) \cdot h_v^{(l-1)} + \sum_{u \in N(v)} h_u^{(l-1)}\right)\]


\subsection{Expressiveness of GNNs}
To assess the expressiveness of a GNN we have to look at its \textbf{computational graph}. It is important to keep in mind that the computational graph can only distinguish different node features not node IDs. Therefore, they are identical to subtrees rooted at the respective nodes.\newline
Hence, we want GNNs to \textbf{map subtrees injectively}. This means we need to retain information about neighborhood structure. As the same feature yields to the same message we achieve this by using \textbf{injectiv aggregation functions}.

\subsubsection{Mean - Pool}
Cannot distinguish different multi-sets with the same embedding proportion.

\subsubsection{Max - Pool}
Cannot distinguish different sets with the same embeddings.

\subsubsection{Injective Multi-Set Function}
Any multi-set function can be expressed as:
\[\phi\left(\sum_{x \in X} f(x)\right)\]
According to the \textbf{universal approximation theorem} \cite{HORNIK1989359}, any function $\phi$ and $f$ can be modeled with an MLP. As MLPs can model compositions of functions, we end up with:
\[MLP\left(\sum_{x \in X} x\right)\]
Transferring this to the domain of GNNs it yields \textbf{GIN(\ref{GIN}) as the most expressive GNN}.


\section{GNN Training Pipeline}

\subsection{Preprocessing}

\subsubsection{Graph Augmentation}
\begin{itemize}
	\item \textbf{Lack Of Features}
		\vspace{-9pt}
		\begin{itemize}[noitemsep]
			\item Constant node features (low expressiveness, high generalization to new nodes).
			\item One - hot node features (high expressiveness, no generalization to new nodes).
			\item Almost any node property.
		\end{itemize}
	\item \textbf{Sparse/Dense/Large Graph Structure}
		\vspace{-9pt}
		\begin{itemize}[noitemsep]
			\item Sparse: add virtual edges / node
			\item Dense: sample neighbors
			\item Large: sample subgraphs
		\end{itemize}
\end{itemize}

\subsubsection{Dataset Split}
We distinguish between \textbf{\gls{Fixed Split}} and \textbf{\gls{Random Split}}.
The problem is that the \textbf{components} of a graph are \textbf{not independent} from one another.
\begin{itemize}
	\item[] \textbf{Transductive Setting} \newline
		The model can observe the \textbf{whole graph} but we \textbf{split the labels} we train / validate / test on. $\rightarrow$ Not applicable for graph - level predictions.
	\item[] \textbf{Inductive Setting} \newline
		We \textbf{break} the graph into \textbf{independent graphs} we train / validate / test on.
\end{itemize}

\subsection{Training}

\subsubsection{Training Settings}
\begin{itemize}[noitemsep]
	\item Supervised Learning vs. Unsupervised Learning
	\item What do we want to predict?
	\begin{itemize}[noitemsep]
		\item Node - Level
		\item Edge - Level
		\item Graph - Level
	\end{itemize}
\end{itemize}

To make a \textbf{k-way prediction} $\hat{y}$ on any level, we need \textbf{prediction heads}. K-way prediction means classification with k classes or regression with k targets.

\subsubsection{Node - Level}
After running the GNN, we have embedding $h_v \in \mathbb{R}^d$ for node v. For node - level prediction we can therefore simply use a matrix transformation:
\[\hat{y}_v = Head_{node}(h_v) = Wh_v\]
\[W \in \mathbb{R}^{k \times d}\]

\subsubsection{Edge - Level}
For edge - level predictions, the prediction head takes \textbf{two embeddings} as input. \newline
Examples:
\begin{itemize}
	\item $\hat{y}_{uv} = Linear(Concat(h_u, h_v))$ \newline
		not commutative $\rightarrow$ problematic for undirected graphs
	\item 1-way prediction: $\hat{y}_{uv} = h_u \bullet h_v$ \newline
		k-way prediction ($\approx$ Multi-Head Attention):
		\[\hat{y}_{uv}^{(i)} = h_u^TW^{(i)}h_v\]
		\[\hat{y}_{uv} = Concat(\{\hat{y}_{uv}^{(i)} | 1 \leqslant i \leqslant k\})\]
\end{itemize}

\subsubsection{Graph - Level}
For graph - level predictions, the prediction head takes \textbf{all embeddings} as input. It therefore basically is very similar to an \textbf{aggregation function}. \newline
\begin{itemize}
	\item \textbf{Global Pooling}: similar to GNN aggregation functions (like Mean, Max and Sum) $\rightarrow$ same problems occur.
	\item \textbf{Hierarchical Pooling}: aggregate embeddings inside \textbf{clusters} of the graph and repeat until the result is a single embedding.
\end{itemize}

\section{Graph Generation}

\subsection{Network Properties}

\subsubsection{Degree Distribution}
The degree distribution \textbf{$P(k)$} is the probability that a random node has degree k.\newline
Computation:
\[N_k = \text{\#nodes with degree k}\]
\[P(k) = \frac{N_k}{N}\]

\subsubsection{Clustering Coefficient}
How connected the neighborhood of node $i$ with degree $k_i$ is. It basically measures, how many edges out of all possible edges exist.
\[C_i = \frac{2e_i}{k_i\left(k_i-1\right)}\]
\[C = \frac{1}{N} \sum_{i}^{N} C_i\]
\begin{center}
\begin{tabular}{l c l}
	$C_i$ & \dots & clustering coefficient \\
	$e_i$ & \dots & total number of edges in neighborhood of node $i$
\end{tabular}
\end{center}

\subsubsection{Connectivity}
Connectivity is the size of the \textbf{largest connected component}.
This largest component is called the \textbf{giant component}.
Can be found by i.e. BFS.

\subsubsection{Path Length}
\begin{itemize}
	\item[] \textbf{Diameter}\newline
		\[max_{u,v}d(u,v)\]
		\begin{center}
			$d(u,v)$ \dots length of shortest path between u and v
		\end{center}
	\item[] \textbf{Average Path Length}\newline
		\[\overline{h} = \frac{1}{2E_{max}}\sum_{i,j \neq i} h_{ij}\]
		\begin{center}
		\begin{tabular}{l c l}
			$E_{max}$ & \dots & max number of edges $\left(\frac{n(n-1)}{2}\right)$ \\
			$h_{ij}$ & \dots & $d(i,j)$
		\end{tabular}
		\end{center}
\end{itemize}

\subsubsection{Expansion}
Expansion $\alpha$ is defined as
\[\alpha = \min_{S \subseteq V}\frac{\text{\#edges leaving S}}{min(\vert S \vert, \vert V \setminus S \vert)}\]
and can be interpreted as the minimum average number of edges that ties a node $u$ of set $S$ to the set $V \setminus S$. This corresponds to a kind of \textbf{robustness of the graph}.

\subsection{Erdös-Renyi Random Graphs}
An Erdös-Renyi Random Graph \textbf{$G_{np}$} is an undirected graph with \textbf{n nodes}. Each \textbf{edge (u,v)} has probability \textbf{$p$} to exist.

\subsubsection{Degree Distribution}
The degree distribution \textbf{$P(k)$} for each node is \textbf{binomial}. We use $n-1$ instead of $n$ as we don't allow self loops.
\[P(k) = \binom{n-1}{k}p^k(1-p)^{n-1-k}\]

\subsubsection{Clustering Coefficient}
As the \# edge distribution is binomial:
\[\mathbf{E}[e_i] = p \cdot \frac{k_i(k_i - 1)}{2}\]
\[\Longrightarrow \mathbf{E}[C_i] = \frac{2\mathbf{E}[e_i]}{k_i(k_i-1)} = \frac{p \cdot k_i(k_i-1)}{k_i(k_i-1} = p = \frac{\overline{k}}{n-1} \approx \frac{\overline{k}}{n}\]

\subsection{Small-World Model}
Small world models try to achieve \textbf{low diameter} while retaining a \textbf{high clustering coefficient}.
\begin{itemize}
	\item start with regular grid $\Rightarrow$ high clustering coefficient
	\item change target of some edges randomly $\Rightarrow$ low diameter
\end{itemize}

\subsection{Kronecker Graphs \cite{leskovec2009kronecker}}

\subsubsection{Kronecker Product}
Let A and B be matrices.
\[C = A \otimes B = 
\begin{pmatrix}
a_{1,1}B & \cdots & a_{1,m}B \\
\vdots & \vdots & \vdots \\
a_{n,1}B & \cdots & a_{n,m}B
\end{pmatrix}\]
For \textbf{recursive graph structurs} we apply the Kronecker Product \textbf{iteratively}. Different matrices $K_1$, $K_1^{'}$, $\ldots$ are possible.
\[K_1^{m} = K_m = K_{m-1} \otimes K_1\]

\subsubsection{Stochastic Kronecker Graphs}
Idea: use \textbf{probability matrix} and compute the Kronecker Product.

Problem: $n^2$ probabilities.

Solution: Select $m$ edges randomly based on the probability matrix by \textbf{iteratively sampling a pair} column/row $(u,v)$.\newline
When going $1 \leqslant i \leqslant q$ layers deep, the graph has $n = 2^q$ nodes and one can formalize the algorithm as:
\begin{algorithm}
\caption{Generating Stochastic Kronecker Graph Edge}
\begin{algorithmic}
	\State $\Theta$ \Comment{probability matrix}
	\State $x = 0, y = 0$ \Comment{edge location}
	\For{$i \gets 1$ to $q$}
		\State pick random $(u,v)$ with probability $\Theta_{uv}$
		\State $x \gets x + u \cdot 2^{q-i}$
		\State $y \gets y + v \cdot 2^{q-1}$
	\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{GraphRNN \cite{you2018graphrnn}}
We want $p_{model}(x \vert \theta)$ to model the data distribution $p_{data}(x)$. $\rightarrow$ \textbf{Maximum Likelihood}
\[\theta^* = \argmax_{\theta} \mathbf{E}_{x \thicksim p_{data}} \log p_{model}(x \vert \theta)\]
\textbf{GraphRNN} consists of two RNNs
\begin{itemize}
	\item[] Node - Level RNN
	\item[] Edge - Level RNN
\end{itemize}
which are connected the way as illustrated in \cref{fig:GraphRNN0}.
We look at each edge level RNN Cell's output as \textbf{a probability to sample from}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{GraphRNN_Schema}
	\caption{GraphRNN Schema \cite{you2018graphrnn}}
	\label{fig:GraphRNN0}
\end{figure}

It is worth noting that the graph state is updated as:
\[h_i = f_{trans}(h_{i-1},S_{i-1}^{\pi})\]
\begin{center}
\begin{tabular}{l c l}
	$f_{trans}$ & \dots & transition module (NN) \\
	$S_{i-1}^{\pi}$ & \dots & last entry in adjacency vector of node $\pi (v_{i-1})$
\end{tabular}
\end{center}
Improvement: Only train on $BFS(G, \pi)$.\newline 
$\rightarrow$ less BFS orderings than node permutations\newline
$\rightarrow$ edge - level RNN only considers edges to the nodes in the previous level of BFS

\newpage
\printnoidxglossary

\newpage
\printbibliography

\end{document}
